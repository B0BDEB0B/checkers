
```{r, setup, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  tidy = FALSE,
  error = FALSE,
  fig.width = 8,
  fig.height = 8)
```


[![Project Status: WIP - Initial development is in progress, but there has not yet been a stable, usable release suitable for the public.](http://www.repostatus.org/badges/latest/wip.svg)](http://www.repostatus.org/#wip)
[![MIT Licensed - Copyright 2016 Noam Ross and Jenny Bryan](https://img.shields.io/badge/license-MIT-blue.svg)](https://badges.mit-license.org/)
[![Linux Build Status](https://travis-ci.org/noamross/checkers.svg?branch=master)](https://travis-ci.org/noamross/checkers)
[![Coverage Status](https://img.shields.io/codecov/c/github/ropenscilabs/checkers/master.svg)](https://codecov.io/github/ropenscilabs/checkers?branch=master)

# checkers

Checkers is a framework for reviewing and automated checking of best practices
for [research compendia](https://github.com/ropensci/rrrpkg)

## Goals

* What is the equivalent of R CMD check? What is the equivalent to the ROpenSci package onboarding process? What is code coverage for data?
* What can be automated? What needs human review?
* Working product vs. final product?
* What are the typical parts/phases of an analysis? This can help you know where you are. How do you get there from where you are now?
The carrot (i.e a sticker/badge) vs. the stick.
* Can you evaluate the git commit messages?
* How can I assess my analysis process from data to code to analysis to reporting?

## Phases of Analysis

Types of Test/Checks/Assessment groups 

* Data 
* Code 
* Model/Analysis 
* Visualization

checkers

## Installation

```{r eval = FALSE}
devtools::install_github("noamross/checkers")
```

## Usage

```{r eval=FALSE}
library(checkers)
```

## License

MIT + file LICENSE Â© 
